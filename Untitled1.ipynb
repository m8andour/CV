{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPz3Je8+9aH63JQZcCNpUgg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gDtv896cEy0F","executionInfo":{"status":"ok","timestamp":1746794722972,"user_tz":-180,"elapsed":32419,"user":{"displayName":"Diaa Tharwat","userId":"03118399692929242656"}},"outputId":"564e5e0f-1082-4d87-aca8-82cbd09d8c17"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Google Drive mounted successfully at /content/drive.\n","\n","Google Drive output directory checked/created: /content/drive/MyDrive/my_fashion_cnn_from_scratch_outputs\n"]}],"source":["import os\n","import zipfile\n","import pandas as pd\n","import numpy as np\n","from PIL import Image\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n","\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    print(\"Google Drive mounted successfully at /content/drive.\")\n","except Exception as e:\n","    print(f\"Could not mount Google Drive: {e}\")\n","    print(\"Proceeding without Google Drive mount. You will not be able to save files permanently.\")\n","\n","\n","GDRIVE_SAVE_DIR = '/content/drive/MyDrive/my_fashion_cnn_from_scratch_outputs'\n","os.makedirs(GDRIVE_SAVE_DIR, exist_ok=True)\n","print(f\"\\nGoogle Drive output directory checked/created: {GDRIVE_SAVE_DIR}\")"]},{"cell_type":"code","source":["!pip install kaggle\n","print(\"Kaggle library installed.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lNKubIcsFFZi","executionInfo":{"status":"ok","timestamp":1746794736602,"user_tz":-180,"elapsed":3447,"user":{"displayName":"Diaa Tharwat","userId":"03118399692929242656"}},"outputId":"59a1f539-fbeb-424a-d8fb-46c6dcb2d83e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.2)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n","Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.4.26)\n","Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.1)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.4)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n","Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n","Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n","Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n","Kaggle library installed.\n"]}]},{"cell_type":"code","source":["import os\n","\n","uploaded_kaggle_json_path = '/content/kaggle.json'\n","\n","print(f\"\\nChecking for kaggle.json at {uploaded_kaggle_json_path}...\")\n","if os.path.exists(uploaded_kaggle_json_path):\n","    print(f\"Found kaggle.json at {uploaded_kaggle_json_path}.\")\n","\n","    kaggle_dir = os.path.join(os.path.expanduser(\"~\"), \".kaggle\")\n","    os.makedirs(kaggle_dir, exist_ok=True)\n","\n","    destination_path = os.path.join(kaggle_dir, \"kaggle.json\")\n","    !cp \"{uploaded_kaggle_json_path}\" \"{destination_path}\"\n","\n","    !chmod 600 \"{destination_path}\"\n","\n","    print(\"Kaggle API key setup complete using manually uploaded file.\")\n","else:\n","    print(f\"Error: kaggle.json NOT found at {uploaded_kaggle_json_path}.\")\n","    print(\"Please perform the manual upload step described in the comments above BEFORE running this cell.\")\n","    print(\"Ensure the file is named exactly 'kaggle.json' and uploaded to the main /content/ directory.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1LnGCvsbFJWl","executionInfo":{"status":"ok","timestamp":1746794750455,"user_tz":-180,"elapsed":216,"user":{"displayName":"Diaa Tharwat","userId":"03118399692929242656"}},"outputId":"098f874c-8cd6-4cac-bf62-cef2910e27e1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Checking for kaggle.json at /content/kaggle.json...\n","Found kaggle.json at /content/kaggle.json.\n","Kaggle API key setup complete using manually uploaded file.\n"]}]},{"cell_type":"code","source":["import os\n","\n","dataset_name = 'paramaggarwal/fashion-product-images-small'\n","data_dir = '/content/fashion_data'\n","\n","os.makedirs(data_dir, exist_ok=True)\n","\n","print(f\"\\nDownloading dataset '{dataset_name}' to '{data_dir}'...\")\n","!kaggle datasets download -d {dataset_name} -p {data_dir} --unzip\n","print(\"Download and extraction process initiated.\")\n","\n","\n","csv_path = os.path.join(data_dir, 'styles.csv')\n","images_dir = os.path.join(data_dir, 'images')\n","\n","print(f\"\\nExpected CSV file path: {csv_path}\")\n","print(f\"Expected images directory: {images_dir}\")\n","print(\"\\nReminder: Data in /content/ is temporary and will be deleted when the Colab session ends.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uq5SS--LFMd5","executionInfo":{"status":"ok","timestamp":1746794803786,"user_tz":-180,"elapsed":39884,"user":{"displayName":"Diaa Tharwat","userId":"03118399692929242656"}},"outputId":"f8151b22-c06a-40a9-ef00-e959ef7868f3"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Downloading dataset 'paramaggarwal/fashion-product-images-small' to '/content/fashion_data'...\n","Dataset URL: https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small\n","License(s): MIT\n","Download and extraction process initiated.\n","\n","Expected CSV file path: /content/fashion_data/styles.csv\n","Expected images directory: /content/fashion_data/images\n","\n","Reminder: Data in /content/ is temporary and will be deleted when the Colab session ends.\n"]}]},{"cell_type":"code","source":["import os\n","if 'data_dir' in locals() and os.path.exists(data_dir) and \\\n","   'csv_path' in locals() and 'images_dir' in locals():\n","\n","    print(f\"\\nListing contents of download directory: {data_dir}\")\n","    !ls -l \"{data_dir}\"\n","\n","    print(f\"\\nChecking for existence of CSV file at: {csv_path}\")\n","    if os.path.exists(csv_path):\n","        print(f\"Success: {csv_path} found!\")\n","    else:\n","        print(f\"Error: {csv_path} NOT found at the expected path.\")\n","        print(\"Possible issue: Download/unzip failed in Cell 4 or file unzipped to a subfolder.\")\n","        print(\"Action: Check the output of the 'ls' command above. If the CSV is missing or in a subfolder, re-run Cell 4 and check its output for errors.\")\n","\n","\n","    print(f\"\\nChecking for existence of images directory at: {images_dir}\")\n","    if os.path.exists(images_dir):\n","        print(f\"Success: {images_dir} found!\")\n","    else:\n","        print(f\"Error: {images_dir} NOT found at the expected path.\")\n","        print(\"Action: If this folder is missing, re-run Cell 4 and check its output for errors.\")\n","\n","else:\n","    print(\"Error: Download directory, csv_path, or images_dir not properly defined or accessible. Please check Cells 3 and 4.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UwFabAqVFZ0L","executionInfo":{"status":"ok","timestamp":1746794817539,"user_tz":-180,"elapsed":148,"user":{"displayName":"Diaa Tharwat","userId":"03118399692929242656"}},"outputId":"d31da4c7-3cdc-415e-9f60-04780503ad34"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Listing contents of download directory: /content/fashion_data\n","total 5364\n","drwxr-xr-x 2 root root 1155072 May  9 12:46 images\n","drwxr-xr-x 3 root root    4096 May  9 12:46 myntradataset\n","-rw-r--r-- 1 root root 4332000 May  9 12:46 styles.csv\n","\n","Checking for existence of CSV file at: /content/fashion_data/styles.csv\n","Success: /content/fashion_data/styles.csv found!\n","\n","Checking for existence of images directory at: /content/fashion_data/images\n","Success: /content/fashion_data/images found!\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","if 'csv_path' in locals() and os.path.exists(csv_path):\n","    try:\n","        df = pd.read_csv(csv_path, on_bad_lines='skip')\n","        print(\"CSV file loaded successfully (bad lines skipped).\")\n","\n","        print(\"\\nFirst 5 rows of the dataframe:\")\n","        print(df.head())\n","\n","        print(\"\\nDataframe Info:\")\n","        df.info()\n","\n","        print(\"\\nExploring 'articleType' column:\")\n","        num_article_types = df['articleType'].nunique()\n","        print(f\"Number of unique article types: {num_article_types}\")\n","\n","        print(\"\\nTop 20 most frequent article types:\")\n","        print(df['articleType'].value_counts().head(20))\n","\n","    except Exception as e:\n","        print(f\"An error occurred while loading or exploring CSV: {e}\")\n","        df = None\n","else:\n","    print(f\"Error: CSV file not found at {csv_path} or csv_path not defined. Please check Cell 5.\")\n","    df = None"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OFzuKEOMFedi","executionInfo":{"status":"ok","timestamp":1746794836728,"user_tz":-180,"elapsed":496,"user":{"displayName":"Diaa Tharwat","userId":"03118399692929242656"}},"outputId":"7f6d83a3-8581-43d0-83a2-437cf94e8f3d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["CSV file loaded successfully (bad lines skipped).\n","\n","First 5 rows of the dataframe:\n","      id gender masterCategory subCategory  articleType baseColour  season  \\\n","0  15970    Men        Apparel     Topwear       Shirts  Navy Blue    Fall   \n","1  39386    Men        Apparel  Bottomwear        Jeans       Blue  Summer   \n","2  59263  Women    Accessories     Watches      Watches     Silver  Winter   \n","3  21379    Men        Apparel  Bottomwear  Track Pants      Black    Fall   \n","4  53759    Men        Apparel     Topwear      Tshirts       Grey  Summer   \n","\n","     year   usage                             productDisplayName  \n","0  2011.0  Casual               Turtle Check Men Navy Blue Shirt  \n","1  2012.0  Casual             Peter England Men Party Blue Jeans  \n","2  2016.0  Casual                       Titan Women Silver Watch  \n","3  2011.0  Casual  Manchester United Men Solid Black Track Pants  \n","4  2012.0  Casual                          Puma Men Grey T-shirt  \n","\n","Dataframe Info:\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 44424 entries, 0 to 44423\n","Data columns (total 10 columns):\n"," #   Column              Non-Null Count  Dtype  \n","---  ------              --------------  -----  \n"," 0   id                  44424 non-null  int64  \n"," 1   gender              44424 non-null  object \n"," 2   masterCategory      44424 non-null  object \n"," 3   subCategory         44424 non-null  object \n"," 4   articleType         44424 non-null  object \n"," 5   baseColour          44409 non-null  object \n"," 6   season              44403 non-null  object \n"," 7   year                44423 non-null  float64\n"," 8   usage               44107 non-null  object \n"," 9   productDisplayName  44417 non-null  object \n","dtypes: float64(1), int64(1), object(8)\n","memory usage: 3.4+ MB\n","\n","Exploring 'articleType' column:\n","Number of unique article types: 143\n","\n","Top 20 most frequent article types:\n","articleType\n","Tshirts                  7067\n","Shirts                   3217\n","Casual Shoes             2845\n","Watches                  2542\n","Sports Shoes             2036\n","Kurtas                   1844\n","Tops                     1762\n","Handbags                 1759\n","Heels                    1323\n","Sunglasses               1073\n","Wallets                   936\n","Flip Flops                914\n","Sandals                   897\n","Briefs                    849\n","Belts                     813\n","Backpacks                 724\n","Socks                     686\n","Formal Shoes              637\n","Perfume and Body Mist     613\n","Jeans                     609\n","Name: count, dtype: int64\n"]}]},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","import os\n","\n","if 'df' in locals() and df is not None and 'images_dir' in locals() and os.path.exists(images_dir):\n","    top_n = 20\n","\n","    top_article_types = df['articleType'].value_counts().nlargest(top_n).index.tolist()\n","    print(f\"\\nFocusing on the top {top_n} article types.\")\n","\n","    df_filtered = df[df['articleType'].isin(top_article_types)].reset_index(drop=True)\n","    print(f\"Dataframe size after filtering: {len(df_filtered)}\")\n","\n","    df_filtered['image_path'] = df_filtered['id'].apply(lambda x: os.path.join(images_dir, f\"{x}.jpg\"))\n","\n","    df_processed = df_filtered[df_filtered['image_path'].apply(os.path.exists)].reset_index(drop=True)\n","    print(f\"Dataframe size after checking image paths: {len(df_processed)}\")\n","\n","    le = LabelEncoder()\n","    df_processed['articleType_encoded'] = le.fit_transform(df_processed['articleType'])\n","\n","    label_map = dict(zip(le.transform(le.classes_), le.classes_))\n","    print(\"Label encoding map created.\")\n","\n","    num_classes = len(label_map)\n","    print(f\"Final number of classes for the model: {num_classes}\")\n","\n","else:\n","    print(\"Cannot process data. Dependencies (df, images_dir) missing or invalid. Check Cells 5 and 6.\")\n","    df_processed = None\n","    label_map = None\n","    num_classes = 0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P2JrFgYsFh6K","executionInfo":{"status":"ok","timestamp":1746794852169,"user_tz":-180,"elapsed":1308,"user":{"displayName":"Diaa Tharwat","userId":"03118399692929242656"}},"outputId":"1a73d38c-7377-4ec5-f6bd-20fbb18a1542"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Focusing on the top 20 article types.\n","Dataframe size after filtering: 33146\n","Dataframe size after checking image paths: 33142\n","Label encoding map created.\n","Final number of classes for the model: 20\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","if 'df_processed' in locals() and df_processed is not None and len(df_processed) > 0 and 'num_classes' in locals() and num_classes > 0:\n","    print(\"\\nSplitting data into training and validation sets...\")\n","    train_df, val_df = train_test_split(\n","        df_processed,\n","        test_size=0.2,\n","        random_state=42,\n","        stratify=df_processed['articleType_encoded']\n","    )\n","\n","    print(f\"Training data size: {len(train_df)}\")\n","    print(f\"Validation data size: {len(val_df)}\")\n","else:\n","    print(\"Cannot split data as df_processed is not available or is empty. Check Cell 7.\")\n","    train_df, val_df = None, None"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b8FEjvD3Flz8","executionInfo":{"status":"ok","timestamp":1746794867434,"user_tz":-180,"elapsed":58,"user":{"displayName":"Diaa Tharwat","userId":"03118399692929242656"}},"outputId":"7f1fb92b-aede-4d7f-e9f4-44e9b9c3699d"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Splitting data into training and validation sets...\n","Training data size: 26513\n","Validation data size: 6629\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","IMG_HEIGHT = 224\n","IMG_WIDTH = 224\n","BATCH_SIZE = 32\n","\n","if 'train_df' in locals() and train_df is not None and 'val_df' in locals() and val_df is not None and 'num_classes' in locals() and num_classes > 0:\n","\n","    print(\"\\nSetting up Data Generators...\")\n","    train_datagen = ImageDataGenerator(\n","        rescale=1./255,\n","        rotation_range=15, width_shift_range=0.1, height_shift_range=0.1,\n","        shear_range=0.1, zoom_range=0.1, horizontal_flip=True, fill_mode='nearest'\n","    )\n","\n","    val_datagen = ImageDataGenerator(rescale=1./255)\n","\n","    train_generator = train_datagen.flow_from_dataframe(\n","        dataframe=train_df,\n","        x_col='image_path',\n","        y_col='articleType',\n","        target_size=(IMG_HEIGHT, IMG_WIDTH),\n","        batch_size=BATCH_SIZE,\n","        class_mode='categorical',\n","        shuffle=True\n","    )\n","\n","    val_generator = val_datagen.flow_from_dataframe(\n","        dataframe=val_df,\n","        x_col='image_path',\n","        y_col='articleType',\n","        target_size=(IMG_HEIGHT, IMG_WIDTH),\n","        batch_size=BATCH_SIZE,\n","        class_mode='categorical',\n","        shuffle=False\n","    )\n","\n","    generator_class_indices = train_generator.class_indices\n","    generator_label_map = {v: k for k, v in generator_class_indices.items()}\n","\n","    print(\"\\nData generators created successfully.\")\n","\n","    steps_per_epoch = train_generator.n // train_generator.batch_size\n","    validation_steps = val_generator.n // val_generator.batch_size\n","    print(f\"\\nSteps per epoch: {steps_per_epoch}\")\n","    print(f\"Validation steps: {validation_steps}\")\n","\n","else:\n","    print(\"Cannot create generators. Training/validation data or number of classes not available. Check Cells 8.\")\n","    train_generator, val_generator = None, None\n","    generator_label_map = None\n","    steps_per_epoch = 0\n","    validation_steps = 0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ffv7qI8HFoVg","executionInfo":{"status":"ok","timestamp":1746794877730,"user_tz":-180,"elapsed":206,"user":{"displayName":"Diaa Tharwat","userId":"03118399692929242656"}},"outputId":"4d62348e-a93a-4ad3-8ab6-5e1d336bd535"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Setting up Data Generators...\n","Found 26513 validated image filenames belonging to 20 classes.\n","Found 6629 validated image filenames belonging to 20 classes.\n","\n","Data generators created successfully.\n","\n","Steps per epoch: 828\n","Validation steps: 207\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n","\n","if 'num_classes' in locals() and num_classes > 0 and 'IMG_HEIGHT' in locals():\n","    print(\"\\nBuilding custom CNN model from scratch...\")\n","\n","    model = Sequential([\n","        Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n","        MaxPooling2D((2, 2)),\n","\n","        Conv2D(64, (3, 3), activation='relu'),\n","        MaxPooling2D((2, 2)),\n","\n","        Conv2D(128, (3, 3), activation='relu'),\n","        MaxPooling2D((2, 2)),\n","\n","        Flatten(),\n","\n","        Dense(128, activation='relu'),\n","        Dropout(0.5),\n","\n","        Dense(num_classes, activation='softmax')\n","    ])\n","\n","    model.summary()\n","    print(\"Custom CNN model built successfully.\")\n","else:\n","    print(\"Cannot build model. Number of classes or image dimensions not defined/valid. Check Cells 8 and 9.\")\n","    model = None"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":577},"id":"V7ptw0dJFtRU","executionInfo":{"status":"ok","timestamp":1746794899890,"user_tz":-180,"elapsed":2970,"user":{"displayName":"Diaa Tharwat","userId":"03118399692929242656"}},"outputId":"68895b09-780f-460d-a3d2-7694bf5bec55"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Building custom CNN model from scratch...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m86528\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │    \u001b[38;5;34m11,075,712\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │         \u001b[38;5;34m2,580\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">86528</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">11,075,712</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,580</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,171,540\u001b[0m (42.62 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,171,540</span> (42.62 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,171,540\u001b[0m (42.62 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,171,540</span> (42.62 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Custom CNN model built successfully.\n"]}]},{"cell_type":"code","source":["if 'model' in locals() and model is not None:\n","    print(\"\\nCompiling model...\")\n","    model.compile(optimizer='adam',\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    print(\"Model compiled successfully.\")\n","else:\n","    print(\"Cannot compile model as it was not built. Check Cell 10.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vLPHx_pQFyLM","executionInfo":{"status":"ok","timestamp":1746794916674,"user_tz":-180,"elapsed":25,"user":{"displayName":"Diaa Tharwat","userId":"03118399692929242656"}},"outputId":"c08b5880-6cf0-4db7-f8d4-6a090eff1b07"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Compiling model...\n","Model compiled successfully.\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","import matplotlib.pyplot as plt\n","import os\n","\n","if 'GDRIVE_SAVE_DIR' in locals() and os.path.exists(GDRIVE_SAVE_DIR):\n","    checkpoint_filename = 'cnn_from_scratch_best_weights_{epoch:02d}_{val_accuracy:.4f}.weights.h5'\n","    checkpoint_filepath_drive = os.path.join(GDRIVE_SAVE_DIR, checkpoint_filename)\n","\n","    model_checkpoint_callback_drive = ModelCheckpoint(\n","        filepath=checkpoint_filepath_drive,\n","        save_weights_only=True,\n","        monitor='val_accuracy',\n","        mode='max',\n","        save_best_only=True,\n","        verbose=1\n","    )\n","\n","    early_stopping_callback = EarlyStopping(\n","        monitor='val_accuracy',\n","        patience=10,\n","        mode='max',\n","        restore_best_weights=True\n","    )\n","\n","    epochs = 100\n","\n","    if 'model' in locals() and model is not None and 'train_generator' in locals() and train_generator is not None and 'val_generator' in locals() and val_generator is not None and steps_per_epoch > 0 and validation_steps >= 0:\n","        print(f\"\\nStarting training from scratch for a maximum of {epochs} epochs...\")\n","        print(\"Note: Training from scratch takes much longer and may require more epochs.\")\n","\n","        history = model.fit(\n","            train_generator,\n","            steps_per_epoch=steps_per_epoch,\n","            epochs=epochs,\n","            validation_data=val_generator,\n","            validation_steps=validation_steps,\n","            callbacks=[model_checkpoint_callback_drive, early_stopping_callback]\n","        )\n","\n","        print(\"\\nTraining finished.\")\n","\n","        if history:\n","            acc = history.history['accuracy']\n","            val_acc = history.history['val_accuracy']\n","            loss = history.history['loss']\n","            val_loss = history.history['val_loss']\n","            epochs_trained = len(history.history['loss'])\n","            epochs_range = range(epochs_trained)\n","\n","            plt.figure(figsize=(12, 4))\n","            plt.subplot(1, 2, 1)\n","            plt.plot(epochs_range, acc, label='Training Accuracy')\n","            plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","            plt.legend(loc='lower right')\n","            plt.title('Training and Validation Accuracy (From Scratch)')\n","            plt.xlabel(\"Epochs\")\n","            plt.ylabel(\"Accuracy\")\n","\n","            plt.subplot(1, 2, 2)\n","            plt.plot(epochs_range, loss, label='Training Loss')\n","            plt.plot(epochs_range, val_loss, label='Validation Loss')\n","            plt.legend(loc='upper right')\n","            plt.title('Training and Validation Loss (From Scratch)')\n","            plt.xlabel(\"Epochs\")\n","            plt.ylabel(\"Loss\")\n","\n","            plt.show()\n","        else:\n","            print(\"Training history not recorded.\")\n","\n","    else:\n","        print(\"Cannot start training. Model, generators, or step counts are not available. Check previous cells (9, 10, 11).\")\n","        history = None\n","\n","\n","else:\n","    print(\"Google Drive not mounted or save directory not accessible. Cannot save checkpoints.\")\n","    print(\"Training will proceed WITHOUT saving checkpoints to Drive.\")\n","    if 'model' in locals() and model is not None and 'train_generator' in locals() and train_generator is not None and 'val_generator' in locals() and val_generator is not None and steps_per_epoch > 0 and validation_steps >= 0:\n","         print(f\"\\nStarting training from scratch for a maximum of {epochs} epochs (no Drive checkpoints)...\")\n","         history = model.fit(\n","             train_generator,\n","             steps_per_epoch=steps_per_epoch,\n","             epochs=epochs,\n","             validation_data=val_generator,\n","             validation_steps=validation_steps\n","         )\n","         print(\"\\nTraining finished (no Drive checkpoints saved).\")\n","         if history:\n","             acc = history.history['accuracy']\n","             val_acc = history.history['val_accuracy']\n","             loss = history.history['loss']\n","             val_loss = history.history['val_loss']\n","             epochs_trained = len(history.history['loss'])\n","             epochs_range = range(epochs_trained)\n","             plt.figure(figsize=(12, 4))\n","             plt.subplot(1, 2, 1)\n","             plt.plot(epochs_range, acc, label='Training Accuracy')\n","             plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","             plt.legend(loc='lower right')\n","             plt.title('Training and Validation Accuracy (From Scratch)')\n","             plt.xlabel(\"Epochs\")\n","             plt.ylabel(\"Accuracy\")\n","             plt.subplot(1, 2, 2)\n","             plt.plot(epochs_range, loss, label='Training Loss')\n","             plt.plot(epochs_range, val_loss, label='Validation Loss')\n","             plt.legend(loc='upper right')\n","             plt.title('Training and Validation Loss (From Scratch)')\n","             plt.xlabel(\"Epochs\")\n","             plt.ylabel(\"Loss\")\n","             plt.show()\n","         else:\n","              print(\"Training history not recorded.\")\n","    else:\n","         print(\"Cannot start training (even without checkpoints). Dependencies missing.\")\n","         history = None"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zemIVN2BF1km","outputId":"e3828da7-f35c-46f6-9634-6854a584ea0d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Starting training from scratch for a maximum of 100 epochs...\n","Note: Training from scratch takes much longer and may require more epochs.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458ms/step - accuracy: 0.3715 - loss: 2.0931\n","Epoch 1: val_accuracy improved from -inf to 0.75030, saving model to /content/drive/MyDrive/my_fashion_cnn_from_scratch_outputs/cnn_from_scratch_best_weights_01_0.7503.weights.h5\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 471ms/step - accuracy: 0.3717 - loss: 2.0925 - val_accuracy: 0.7503 - val_loss: 0.7345\n","Epoch 2/100\n","\u001b[1m  1/828\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 63ms/step - accuracy: 0.6875 - loss: 0.8661"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n","  self._interrupted_warning()\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 2: val_accuracy did not improve from 0.75030\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 24ms/step - accuracy: 0.6875 - loss: 0.8661 - val_accuracy: 0.7429 - val_loss: 0.7344\n","Epoch 3/100\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 418ms/step - accuracy: 0.6210 - loss: 1.0955\n","Epoch 3: val_accuracy improved from 0.75030 to 0.77582, saving model to /content/drive/MyDrive/my_fashion_cnn_from_scratch_outputs/cnn_from_scratch_best_weights_03_0.7758.weights.h5\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 501ms/step - accuracy: 0.6210 - loss: 1.0955 - val_accuracy: 0.7758 - val_loss: 0.6206\n","Epoch 4/100\n","\u001b[1m  1/828\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 60ms/step - accuracy: 0.5938 - loss: 1.2316\n","Epoch 4: val_accuracy improved from 0.77582 to 0.77899, saving model to /content/drive/MyDrive/my_fashion_cnn_from_scratch_outputs/cnn_from_scratch_best_weights_04_0.7790.weights.h5\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.5938 - loss: 1.2316 - val_accuracy: 0.7790 - val_loss: 0.6200\n","Epoch 5/100\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step - accuracy: 0.7021 - loss: 0.8727\n","Epoch 5: val_accuracy improved from 0.77899 to 0.80631, saving model to /content/drive/MyDrive/my_fashion_cnn_from_scratch_outputs/cnn_from_scratch_best_weights_05_0.8063.weights.h5\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 462ms/step - accuracy: 0.7021 - loss: 0.8727 - val_accuracy: 0.8063 - val_loss: 0.5448\n","Epoch 6/100\n","\u001b[1m  1/828\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 62ms/step - accuracy: 0.8438 - loss: 0.5718\n","Epoch 6: val_accuracy improved from 0.80631 to 0.80827, saving model to /content/drive/MyDrive/my_fashion_cnn_from_scratch_outputs/cnn_from_scratch_best_weights_06_0.8083.weights.h5\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.8438 - loss: 0.5718 - val_accuracy: 0.8083 - val_loss: 0.5324\n","Epoch 7/100\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 405ms/step - accuracy: 0.7255 - loss: 0.7695\n","Epoch 7: val_accuracy improved from 0.80827 to 0.82231, saving model to /content/drive/MyDrive/my_fashion_cnn_from_scratch_outputs/cnn_from_scratch_best_weights_07_0.8223.weights.h5\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 472ms/step - accuracy: 0.7255 - loss: 0.7695 - val_accuracy: 0.8223 - val_loss: 0.5002\n","Epoch 8/100\n","\u001b[1m  1/828\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 56ms/step - accuracy: 0.9375 - loss: 0.3326\n","Epoch 8: val_accuracy did not improve from 0.82231\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9375 - loss: 0.3326 - val_accuracy: 0.8199 - val_loss: 0.5064\n","Epoch 9/100\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 437ms/step - accuracy: 0.7558 - loss: 0.6998\n","Epoch 9: val_accuracy improved from 0.82231 to 0.83530, saving model to /content/drive/MyDrive/my_fashion_cnn_from_scratch_outputs/cnn_from_scratch_best_weights_09_0.8353.weights.h5\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m444s\u001b[0m 483ms/step - accuracy: 0.7558 - loss: 0.6998 - val_accuracy: 0.8353 - val_loss: 0.4449\n","Epoch 10/100\n","\u001b[1m  1/828\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 68ms/step - accuracy: 0.6562 - loss: 0.9687\n","Epoch 10: val_accuracy improved from 0.83530 to 0.83771, saving model to /content/drive/MyDrive/my_fashion_cnn_from_scratch_outputs/cnn_from_scratch_best_weights_10_0.8377.weights.h5\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.6562 - loss: 0.9687 - val_accuracy: 0.8377 - val_loss: 0.4459\n","Epoch 11/100\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401ms/step - accuracy: 0.7626 - loss: 0.6667\n","Epoch 11: val_accuracy improved from 0.83771 to 0.85507, saving model to /content/drive/MyDrive/my_fashion_cnn_from_scratch_outputs/cnn_from_scratch_best_weights_11_0.8551.weights.h5\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 421ms/step - accuracy: 0.7626 - loss: 0.6666 - val_accuracy: 0.8551 - val_loss: 0.3999\n","Epoch 12/100\n","\u001b[1m  1/828\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 56ms/step - accuracy: 0.7812 - loss: 0.8864\n","Epoch 12: val_accuracy improved from 0.85507 to 0.85568, saving model to /content/drive/MyDrive/my_fashion_cnn_from_scratch_outputs/cnn_from_scratch_best_weights_12_0.8557.weights.h5\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.7812 - loss: 0.8864 - val_accuracy: 0.8557 - val_loss: 0.4036\n","Epoch 13/100\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396ms/step - accuracy: 0.7800 - loss: 0.6314\n","Epoch 13: val_accuracy did not improve from 0.85568\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 430ms/step - accuracy: 0.7800 - loss: 0.6314 - val_accuracy: 0.8365 - val_loss: 0.4550\n","Epoch 14/100\n","\u001b[1m  1/828\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 52ms/step - accuracy: 0.7812 - loss: 0.7005\n","Epoch 14: val_accuracy did not improve from 0.85568\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.7812 - loss: 0.7005 - val_accuracy: 0.8297 - val_loss: 0.4765\n","Epoch 15/100\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401ms/step - accuracy: 0.7871 - loss: 0.5993\n","Epoch 15: val_accuracy improved from 0.85568 to 0.85809, saving model to /content/drive/MyDrive/my_fashion_cnn_from_scratch_outputs/cnn_from_scratch_best_weights_15_0.8581.weights.h5\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 441ms/step - accuracy: 0.7871 - loss: 0.5993 - val_accuracy: 0.8581 - val_loss: 0.4070\n","Epoch 16/100\n","\u001b[1m  1/828\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 53ms/step - accuracy: 0.7812 - loss: 0.5909\n","Epoch 16: val_accuracy improved from 0.85809 to 0.85930, saving model to /content/drive/MyDrive/my_fashion_cnn_from_scratch_outputs/cnn_from_scratch_best_weights_16_0.8593.weights.h5\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.7812 - loss: 0.5909 - val_accuracy: 0.8593 - val_loss: 0.4066\n","Epoch 17/100\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - accuracy: 0.7999 - loss: 0.5567\n","Epoch 17: val_accuracy improved from 0.85930 to 0.86866, saving model to /content/drive/MyDrive/my_fashion_cnn_from_scratch_outputs/cnn_from_scratch_best_weights_17_0.8687.weights.h5\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 408ms/step - accuracy: 0.7999 - loss: 0.5567 - val_accuracy: 0.8687 - val_loss: 0.3686\n","Epoch 18/100\n","\u001b[1m  1/828\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 59ms/step - accuracy: 0.7188 - loss: 0.7335\n","Epoch 18: val_accuracy improved from 0.86866 to 0.86926, saving model to /content/drive/MyDrive/my_fashion_cnn_from_scratch_outputs/cnn_from_scratch_best_weights_18_0.8693.weights.h5\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.7188 - loss: 0.7335 - val_accuracy: 0.8693 - val_loss: 0.3673\n","Epoch 19/100\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - accuracy: 0.8064 - loss: 0.5483\n","Epoch 19: val_accuracy did not improve from 0.86926\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 417ms/step - accuracy: 0.8064 - loss: 0.5483 - val_accuracy: 0.8673 - val_loss: 0.3650\n","Epoch 20/100\n","\u001b[1m  1/828\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 62ms/step - accuracy: 0.7500 - loss: 0.8356\n","Epoch 20: val_accuracy did not improve from 0.86926\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.7500 - loss: 0.8356 - val_accuracy: 0.8678 - val_loss: 0.3663\n","Epoch 21/100\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396ms/step - accuracy: 0.8068 - loss: 0.5474\n","Epoch 21: val_accuracy did not improve from 0.86926\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 462ms/step - accuracy: 0.8067 - loss: 0.5475 - val_accuracy: 0.8619 - val_loss: 0.3818\n","Epoch 22/100\n","\u001b[1m  1/828\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 54ms/step - accuracy: 0.7500 - loss: 0.6063\n","Epoch 22: val_accuracy did not improve from 0.86926\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.7500 - loss: 0.6063 - val_accuracy: 0.8585 - val_loss: 0.3960\n","Epoch 23/100\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - accuracy: 0.7991 - loss: 0.5588\n","Epoch 23: val_accuracy did not improve from 0.86926\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m443s\u001b[0m 472ms/step - accuracy: 0.7992 - loss: 0.5588 - val_accuracy: 0.8632 - val_loss: 0.3651\n","Epoch 24/100\n","\u001b[1m  1/828\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 56ms/step - accuracy: 0.8125 - loss: 0.5948\n","Epoch 24: val_accuracy did not improve from 0.86926\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8125 - loss: 0.5948 - val_accuracy: 0.8681 - val_loss: 0.3585\n","Epoch 25/100\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402ms/step - accuracy: 0.8196 - loss: 0.5029\n","Epoch 25: val_accuracy improved from 0.86926 to 0.87243, saving model to /content/drive/MyDrive/my_fashion_cnn_from_scratch_outputs/cnn_from_scratch_best_weights_25_0.8724.weights.h5\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 412ms/step - accuracy: 0.8196 - loss: 0.5029 - val_accuracy: 0.8724 - val_loss: 0.3529\n","Epoch 26/100\n","\u001b[1m  1/828\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 53ms/step - accuracy: 0.7812 - loss: 0.6361\n","Epoch 26: val_accuracy improved from 0.87243 to 0.87274, saving model to /content/drive/MyDrive/my_fashion_cnn_from_scratch_outputs/cnn_from_scratch_best_weights_26_0.8727.weights.h5\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.7812 - loss: 0.6361 - val_accuracy: 0.8727 - val_loss: 0.3534\n","Epoch 27/100\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401ms/step - accuracy: 0.8228 - loss: 0.4906\n","Epoch 27: val_accuracy improved from 0.87274 to 0.87968, saving model to /content/drive/MyDrive/my_fashion_cnn_from_scratch_outputs/cnn_from_scratch_best_weights_27_0.8797.weights.h5\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 423ms/step - accuracy: 0.8228 - loss: 0.4906 - val_accuracy: 0.8797 - val_loss: 0.3408\n","Epoch 28/100\n","\u001b[1m  1/828\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 58ms/step - accuracy: 0.8125 - loss: 0.4583\n","Epoch 28: val_accuracy did not improve from 0.87968\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.8125 - loss: 0.4583 - val_accuracy: 0.8777 - val_loss: 0.3386\n","Epoch 29/100\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step - accuracy: 0.8212 - loss: 0.4981\n","Epoch 29: val_accuracy did not improve from 0.87968\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 417ms/step - accuracy: 0.8212 - loss: 0.4981 - val_accuracy: 0.8702 - val_loss: 0.3503\n","Epoch 30/100\n","\u001b[1m  1/828\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 59ms/step - accuracy: 0.8125 - loss: 0.5097\n","Epoch 30: val_accuracy did not improve from 0.87968\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.8125 - loss: 0.5097 - val_accuracy: 0.8708 - val_loss: 0.3475\n","Epoch 31/100\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - accuracy: 0.8245 - loss: 0.4895\n","Epoch 31: val_accuracy improved from 0.87968 to 0.88028, saving model to /content/drive/MyDrive/my_fashion_cnn_from_scratch_outputs/cnn_from_scratch_best_weights_31_0.8803.weights.h5\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m365s\u001b[0m 408ms/step - accuracy: 0.8245 - loss: 0.4895 - val_accuracy: 0.8803 - val_loss: 0.3301\n","Epoch 32/100\n","\u001b[1m  1/828\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 62ms/step - accuracy: 0.7812 - loss: 0.8538\n","Epoch 32: val_accuracy improved from 0.88028 to 0.88844, saving model to /content/drive/MyDrive/my_fashion_cnn_from_scratch_outputs/cnn_from_scratch_best_weights_32_0.8884.weights.h5\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - accuracy: 0.7812 - loss: 0.8538 - val_accuracy: 0.8884 - val_loss: 0.3152\n","Epoch 33/100\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401ms/step - accuracy: 0.8300 - loss: 0.4717\n","Epoch 33: val_accuracy did not improve from 0.88844\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 459ms/step - accuracy: 0.8300 - loss: 0.4717 - val_accuracy: 0.8860 - val_loss: 0.3240\n","Epoch 34/100\n","\u001b[1m  1/828\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 52ms/step - accuracy: 0.8750 - loss: 0.4372\n","Epoch 34: val_accuracy did not improve from 0.88844\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.8750 - loss: 0.4372 - val_accuracy: 0.8859 - val_loss: 0.3238\n","Epoch 35/100\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396ms/step - accuracy: 0.8348 - loss: 0.4732\n","Epoch 35: val_accuracy did not improve from 0.88844\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m442s\u001b[0m 469ms/step - accuracy: 0.8348 - loss: 0.4732 - val_accuracy: 0.8806 - val_loss: 0.3267\n","Epoch 36/100\n","\u001b[1m  1/828\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 54ms/step - accuracy: 0.8438 - loss: 0.3344\n","Epoch 36: val_accuracy did not improve from 0.88844\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.8438 - loss: 0.3344 - val_accuracy: 0.8782 - val_loss: 0.3296\n","Epoch 37/100\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 409ms/step - accuracy: 0.8335 - loss: 0.4636\n","Epoch 37: val_accuracy improved from 0.88844 to 0.89161, saving model to /content/drive/MyDrive/my_fashion_cnn_from_scratch_outputs/cnn_from_scratch_best_weights_37_0.8916.weights.h5\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m394s\u001b[0m 420ms/step - accuracy: 0.8336 - loss: 0.4636 - val_accuracy: 0.8916 - val_loss: 0.3064\n","Epoch 38/100\n","\u001b[1m  1/828\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 62ms/step - accuracy: 0.7812 - loss: 0.4337\n","Epoch 38: val_accuracy did not improve from 0.89161\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.7812 - loss: 0.4337 - val_accuracy: 0.8912 - val_loss: 0.3075\n","Epoch 39/100\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 410ms/step - accuracy: 0.8434 - loss: 0.4401\n","Epoch 39: val_accuracy did not improve from 0.89161\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 429ms/step - accuracy: 0.8434 - loss: 0.4401 - val_accuracy: 0.8844 - val_loss: 0.3220\n","Epoch 40/100\n","\u001b[1m  1/828\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 53ms/step - accuracy: 0.7812 - loss: 0.6313\n","Epoch 40: val_accuracy did not improve from 0.89161\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.7812 - loss: 0.6313 - val_accuracy: 0.8813 - val_loss: 0.3288\n","Epoch 41/100\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 405ms/step - accuracy: 0.8412 - loss: 0.4409\n","Epoch 41: val_accuracy did not improve from 0.89161\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 415ms/step - accuracy: 0.8412 - loss: 0.4409 - val_accuracy: 0.8836 - val_loss: 0.3240\n","Epoch 42/100\n","\u001b[1m  1/828\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 53ms/step - accuracy: 0.7812 - loss: 0.4713\n","Epoch 42: val_accuracy did not improve from 0.89161\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.7812 - loss: 0.4713 - val_accuracy: 0.8860 - val_loss: 0.3203\n","Epoch 43/100\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407ms/step - accuracy: 0.8421 - loss: 0.4428\n","Epoch 43: val_accuracy did not improve from 0.89161\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 425ms/step - accuracy: 0.8421 - loss: 0.4428 - val_accuracy: 0.8893 - val_loss: 0.3214\n","Epoch 44/100\n","\u001b[1m  1/828\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 53ms/step - accuracy: 0.8438 - loss: 0.3536\n","Epoch 44: val_accuracy did not improve from 0.89161\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.8438 - loss: 0.3536 - val_accuracy: 0.8883 - val_loss: 0.3239\n","Epoch 45/100\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 410ms/step - accuracy: 0.8468 - loss: 0.4338\n","Epoch 45: val_accuracy did not improve from 0.89161\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m368s\u001b[0m 420ms/step - accuracy: 0.8468 - loss: 0.4337 - val_accuracy: 0.8880 - val_loss: 0.3047\n","Epoch 46/100\n","\u001b[1m  1/828\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 54ms/step - accuracy: 0.7188 - loss: 1.0427\n","Epoch 46: val_accuracy did not improve from 0.89161\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.7188 - loss: 1.0427 - val_accuracy: 0.8878 - val_loss: 0.3044\n","Epoch 47/100\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 410ms/step - accuracy: 0.8463 - loss: 0.4307\n","Epoch 47: val_accuracy improved from 0.89161 to 0.89629, saving model to /content/drive/MyDrive/my_fashion_cnn_from_scratch_outputs/cnn_from_scratch_best_weights_47_0.8963.weights.h5\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 430ms/step - accuracy: 0.8463 - loss: 0.4307 - val_accuracy: 0.8963 - val_loss: 0.3001\n","Epoch 48/100\n","\u001b[1m  1/828\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 55ms/step - accuracy: 0.9375 - loss: 0.1731\n","Epoch 48: val_accuracy improved from 0.89629 to 0.89689, saving model to /content/drive/MyDrive/my_fashion_cnn_from_scratch_outputs/cnn_from_scratch_best_weights_48_0.8969.weights.h5\n","\u001b[1m828/828\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9375 - loss: 0.1731 - val_accuracy: 0.8969 - val_loss: 0.3006\n","Epoch 49/100\n","\u001b[1m268/828\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:52\u001b[0m 416ms/step - accuracy: 0.8555 - loss: 0.4139"]}]}]}